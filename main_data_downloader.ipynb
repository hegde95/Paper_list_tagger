{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shashank/research/paper_tagging/Paper_list_tagger'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "project_root = os.path.join('/home/shashank/research/paper_tagging/Paper_list_tagger')\n",
    "os.chdir(project_root)\n",
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import arxiv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import openvpn_api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "file_name = 'USC Robotic Embedded Systems Lab Content - Publications (use this).csv'\n",
    "file_path = os.path.join(project_root, 'data', file_name)\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Authors', 'Type', 'Tags', 'Include?', 'Title', 'Details',\n",
       "       'Bibtex ID', 'PDF ', 'Video', 'ArXiv', 'Codes', 'Slides', 'Poster',\n",
       "       'Twitter', 'Linkedin', 'News', 'Source-News', 'Blog', 'Website',\n",
       "       'Other', 'Source-other'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add an Abstract column to if it doesn't exist\n",
    "if 'Abstract' not in df.columns:\n",
    "    df['Abstract'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to retrieve search results\n"
     ]
    }
   ],
   "source": [
    "def get_google_scholar_abstracts(query, num_results=10):\n",
    "    base_url = \"https://scholar.google.com/scholar\"\n",
    "\n",
    "    # Define user-agent headers to send with HTTP requests, use chrome\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"num\": num_results\n",
    "    }\n",
    "\n",
    "    # Send an HTTP GET request with headers\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        abstracts = []\n",
    "        for result in soup.find_all('div', class_='gs_ri'):\n",
    "            title = result.find('h3', class_='gs_rt').text\n",
    "            abstract = result.find('div', class_='gs_rs')\n",
    "            if abstract:\n",
    "                abstract = abstract.text\n",
    "            else:\n",
    "                abstract = \"No abstract available\"\n",
    "            abstracts.append((title, abstract))\n",
    "\n",
    "        return abstracts, response\n",
    "    else:\n",
    "        print(\"Error: Unable to retrieve search results\")\n",
    "        return None, response\n",
    "\n",
    "# # Example usage:\n",
    "query = df['Title'][1]\n",
    "abstracts, response = get_google_scholar_abstracts(query, num_results=5)\n",
    "# for i, (title, abstract) in enumerate(abstracts, start=1):\n",
    "#     print(f\"Paper {i} - Title: {title}\\nAbstract: {abstract}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m openvpn_api\u001b[39m.\u001b[39;49mVPN(\u001b[39m'\u001b[39;49m\u001b[39mlocalhost\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m7505\u001b[39;49m)\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m~/miniconda3/envs/tag/lib/python3.11/site-packages/openvpn_api/vpn.py:63\u001b[0m, in \u001b[0;36mVPN.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m (socket\u001b[39m.\u001b[39mtimeout, socket\u001b[39m.\u001b[39merror) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mConnectError(\u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "openvpn_api.VPN('localhost', 7505).connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an abstract column, using the get_google_scholar_abstracts function\n",
    "df[\"Abstract\"] = None\n",
    "for i in range(len(df)):\n",
    "    print(i)\n",
    "    query = df['Title'][i]\n",
    "    abstracts, response = get_google_scholar_abstracts(query, num_results=5)\n",
    "    if response.status_code != 200:\n",
    "        \n",
    "    # sleep for 1 second\n",
    "    time.sleep(1)\n",
    "    if len(abstracts) > 0:\n",
    "        df['Abstract'][i] = abstracts[0][1]\n",
    "    else:\n",
    "        df['Abstract'][i] = None\n",
    "\n",
    "# save the dataframe\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter out papers that have corr in Details or have an entry in ArXiv\n",
    "# papers_that_have_corr = df[df[\"Details\"].str.contains(\"CoRR\", case=False) | df[\"ArXiv\"].str.contains(\"arxiv\", case=False)][[\"Details\", \"ArXiv\"]]\n",
    "\n",
    "# # fill out the ArXiv if its empty. Details has CoRR, vol. abs/x.y, ArXiv needs to be filled with https://arxiv.org/abs/x.y\n",
    "# papers_that_have_corr[\"ArXiv\"] = papers_that_have_corr[\"ArXiv\"].fillna(papers_that_have_corr[\"Details\"].str.extract(r'CoRR, vol. (abs/[\\d\\.]+)', expand=False).str.replace(\"abs/\", \"https://arxiv.org/abs/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download papers from Arxiv\n",
    "# for index, row in papers_that_have_corr.iterrows():\n",
    "#     id = row[\"ArXiv\"].split(\"/\")[-1]\n",
    "#     # if id.pdf exists, skip\n",
    "#     if os.path.exists(f\"data/papers/arxiv/{id}.pdf\"):\n",
    "#         continue\n",
    "#     # arxiv.download(row[\"ArXiv\"], dirpath='./papers/arxiv')\n",
    "#     paper = next(arxiv.Search(id_list=[id]).results())\n",
    "#     # Download the PDF to a specified directory with a custom filename.\n",
    "#     paper.download_pdf(dirpath=\"data/papers/arxiv\", filename=f\"{id}.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
